{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис, благодаря которому пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Мне необходимо обучить модель классифицировать комментарии на позитивные и негативные. В моём распоряжении набор данных с разметкой о токсичности правок.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1. Загрузка библиотек и получение информации о датафрейме "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ksenydelik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ksenydelik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Загрузим необходимые библиотеки\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from scipy import stats as st\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from IPython.display import display\n",
    "from lightgbm import LGBMClassifier\n",
    "import re\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159571 non-null  object\n",
      " 1   toxic   159571 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Вывод:</strong>В данных нет пропусков, типы данных правильные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.Проверка дисбаланса клаасов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.898321\n",
      "1    0.101679\n",
      "Name: toxic, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x20ce9bbaa90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_frequency = data['toxic'].value_counts(normalize=bool)\n",
    "print(class_frequency)\n",
    "class_frequency.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Вывод:</strong>В данном датасете мы наблюдаем сильный дисбаланс классов\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3. Лемматизация текстов пр ипомощи WordNet NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#создадим функцию для лемматизации\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(text):\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    return lemmatized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    ex1 = re.sub(r'[^a-zA-Z ]', ' ', text)\n",
    "    ex = \" \".join(ex1.split())\n",
    "    return ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#применим функцию ко всему датафрейму\n",
    "data['lemm_text'] = data['text'].apply(\n",
    "     lambda x: lemmatize(clear_text(x))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   text       159571 non-null  object\n",
      " 1   toxic      159571 non-null  int64 \n",
      " 2   lemm_text  159571 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1Создадим обучающую и тестовую выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(data, test_size=0.1, random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2. Текст трансофрмер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ksenydelik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы: (143613, 150000)\n"
     ]
    }
   ],
   "source": [
    "corpus = data_train['lemm_text']\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords, max_features=150000)\n",
    "tf_idf = count_tf_idf.fit_transform(corpus)\n",
    "\n",
    "print(\"Размер матрицы:\", tf_idf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы: (15958, 150000)\n"
     ]
    }
   ],
   "source": [
    "corpus_test = data_test['lemm_text']\n",
    "\n",
    "count_tf_idf_test = TfidfVectorizer(stop_words=stopwords)\n",
    "tf_idf_test = count_tf_idf.transform(corpus_test)\n",
    "print(\"Размер матрицы:\", tf_idf_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train = data_train['toxic']\n",
    "target_test = data_test['toxic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3. Модель Логистической регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = make_scorer(f1_score , average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_linear=LogisticRegression()\n",
    "param_grid = { \n",
    "    'class_weight': ['balanced', None],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'class_weight': 'balanced'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "CV_lr = GridSearchCV(estimator=model_linear, param_grid=param_grid, n_jobs= -1, cv = 3, scoring=f1)\n",
    "CV_lr.fit(tf_idf, target_train)\n",
    "CV_lr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-мера логистической регрессии: 0.75\n",
      "Wall time: 5.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(random_state=12345, class_weight='balanced') \n",
    "model.fit(tf_idf, target_train)\n",
    "predicted_test = model.predict(tf_idf_test)\n",
    "result = f1_score(target_test, predicted_test)\n",
    "print(\"f1-мера логистической регрессии:\", '{:.2f}'.format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3. Модель Случайного леса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_forest = RandomForestClassifier(random_state=0)\n",
    "param_grid = { \n",
    "    'max_depth' : [15,20],\n",
    "    'class_weight' : ['balanced'],\n",
    "    'n_estimators' : [150]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'class_weight': 'balanced', 'max_depth': 20, 'n_estimators': 150}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "CV_fc = GridSearchCV(estimator=model_forest, param_grid=param_grid,  n_jobs= -1, cv = 3, scoring=f1)\n",
    "CV_fc.fit(tf_idf, target_train)\n",
    "CV_fc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-мера случайного леса: 0.41\n",
      "Wall time: 20.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_forest = RandomForestClassifier(random_state=12345, n_estimators=150, max_depth=20, class_weight='balanced') \n",
    "model_forest.fit(tf_idf, target_train)\n",
    "predicted_forest_test = model_forest.predict(tf_idf_test)\n",
    "result_forest = f1_score(target_test, predicted_forest_test)\n",
    "print(\"f1-мера случайного леса:\", '{:.2f}'.format(result_forest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.4. Модель Решающего дерева"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tree = DecisionTreeClassifier(random_state=0)\n",
    "param_grid = { \n",
    "    'max_depth' : [10,15,20],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 14s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_depth': 20}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "CV_tc = GridSearchCV(estimator=model_tree, param_grid=param_grid,  n_jobs= -1, cv = 3, scoring=f1)\n",
    "CV_tc.fit(tf_idf, target_train)\n",
    "CV_tc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-мера дерева решений: 0.60\n",
      "Wall time: 21.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_tree = DecisionTreeClassifier(random_state=12345, max_depth=20, class_weight='balanced')\n",
    "model_tree.fit(tf_idf, target_train)\n",
    "predicted_tree_test = model_tree.predict(tf_idf_test)\n",
    "result_tree = f1_score(target_test, predicted_tree_test)\n",
    "print(\"f1-мера дерева решений:\", '{:.2f}'.format(result_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.5. Модель Градиентного бустинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-мера дерева решений: 0.74\n",
      "Wall time: 38.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_LGBM = LGBMClassifier(class_weight='balanced', n_estimators=150, max_depth=20, num_leaves=100)\n",
    "model_LGBM.fit(tf_idf, target_train)\n",
    "predicted_LGBM_test = model_LGBM.predict(tf_idf_test)\n",
    "result_LGBM = f1_score(target_test, predicted_LGBM_test)\n",
    "print(\"f1-мера дерева решений:\", '{:.2f}'.format(result_LGBM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном проекте мы построили модели для поиска токсичных комментариев. Тектовые данные были лемматизированы, а также была дана оценка важности слов при помощи TD_IDF. Лучшей моделью является модель логистической регресиии. F1-мера этой модели равна 0,75. Плюсом данной модели также является скорость её работы. Остальные модели показали более низкую f1 меру и работают гораздо дольше. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
